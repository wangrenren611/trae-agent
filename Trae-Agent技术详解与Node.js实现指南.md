# Trae Agent æŠ€æœ¯è¯¦è§£ä¸ Node.js å®ç°æŒ‡å—

## 1. é¡¹ç›®æ¦‚è¿°ä¸æ ¸å¿ƒä»·å€¼

Trae Agent æ˜¯ç”±å­—èŠ‚è·³åŠ¨å¼€å‘çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½è½¯ä»¶å·¥ç¨‹ä»£ç†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒä»·å€¼åœ¨äºæä¾›äº†ä¸€ä¸ª**é€æ˜ã€æ¨¡å—åŒ–ã€ç ”ç©¶å‹å¥½**çš„AIä»£ç†æ¶æ„ï¼Œä¸“é—¨ç”¨äºå¤„ç†é€šç”¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚

### 1.1 æ ¸å¿ƒç‰¹æ€§åˆ†æ

#### ğŸŒŠ Lakeview æ™ºèƒ½æ€»ç»“ç³»ç»Ÿ
Lakeview æ˜¯ Trae Agent çš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€ï¼Œå®ƒæä¾›äº†å¯¹ä»£ç†æ‰§è¡Œæ­¥éª¤çš„ç®€æ´æ™ºèƒ½æ€»ç»“ã€‚è¿™ä¸ªç³»ç»Ÿçš„æŠ€æœ¯è¦ç‚¹åŒ…æ‹¬ï¼š

- **å®æ—¶æ­¥éª¤æ‘˜è¦**ï¼šåœ¨ä»£ç†æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå®æ—¶ç”Ÿæˆæ¯ä¸ªæ­¥éª¤çš„ç®€æ´æ‘˜è¦
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šåŸºäºä»»åŠ¡ä¸Šä¸‹æ–‡å’Œæ‰§è¡Œå†å²ç”Ÿæˆç›¸å…³æ€§å¼ºçš„æ€»ç»“
- **å¯é…ç½®æ€§**ï¼šæ”¯æŒé€šè¿‡é…ç½®å¯ç”¨æˆ–ç¦ç”¨ï¼Œé€‚åº”ä¸åŒä½¿ç”¨åœºæ™¯

#### ğŸ¤– å¤š LLM æä¾›å•†æ”¯æŒ
ç³»ç»Ÿé‡‡ç”¨äº†**ç­–ç•¥æ¨¡å¼**è®¾è®¡ï¼Œæ”¯æŒå¤šç§ LLM æä¾›å•†ï¼š

```python
class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic" 
    AZURE = "azure"
    OLLAMA = "ollama"
    OPENROUTER = "openrouter"
    DOUBAO = "doubao"
    GOOGLE = "google"
```

æ¯ä¸ªæä¾›å•†éƒ½æœ‰ä¸“é—¨çš„å®¢æˆ·ç«¯å®ç°ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¥å£è¿›è¡Œè°ƒç”¨ã€‚

#### ğŸ› ï¸ ä¸°å¯Œçš„å·¥å…·ç”Ÿæ€ç³»ç»Ÿ
Trae Agent æä¾›äº†å®Œæ•´çš„å·¥å…·æ‰§è¡Œæ¡†æ¶ï¼š

- **æ–‡ä»¶ç¼–è¾‘å·¥å…·**ï¼š`str_replace_based_edit_tool`ã€`json_edit_tool`
- **ç³»ç»Ÿäº¤äº’å·¥å…·**ï¼š`bash` å‘½ä»¤æ‰§è¡Œ
- **æ€ç»´é“¾å·¥å…·**ï¼š`sequentialthinking` ç»“æ„åŒ–æ€è€ƒ
- **ä»»åŠ¡å®Œæˆå·¥å…·**ï¼š`task_done` æ˜ç¡®ä»»åŠ¡çŠ¶æ€
- **MCP å·¥å…·æ”¯æŒ**ï¼šModel Context Protocol æ‰©å±•å·¥å…·

## 2. ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ

### 2.1 æ•´ä½“æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        CLI[CLIæ¥å£]
        Interactive[äº¤äº’æ¨¡å¼]
        Config[é…ç½®ç³»ç»Ÿ]
    end
    
    subgraph "ä»£ç†æ ¸å¿ƒå±‚"
        BaseAgent[BaseAgentåŸºç±»]
        TraeAgent[TraeAgentå®ç°]
        AgentExecution[æ‰§è¡Œå¼•æ“]
    end
    
    subgraph "LLMå®¢æˆ·ç«¯å±‚"
        LLMClient[LLMå®¢æˆ·ç«¯]
        OpenAI[OpenAIå®¢æˆ·ç«¯]
        Anthropic[Anthropicå®¢æˆ·ç«¯]
        Google[Googleå®¢æˆ·ç«¯]
        Others[å…¶ä»–æä¾›å•†...]
    end
    
    subgraph "å·¥å…·æ‰§è¡Œå±‚"
        ToolExecutor[å·¥å…·æ‰§è¡Œå™¨]
        DockerToolExecutor[Dockerå·¥å…·æ‰§è¡Œå™¨]
        Tools[å·¥å…·é›†åˆ]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        TrajectoryRecorder[è½¨è¿¹è®°å½•å™¨]
        DockerManager[Dockerç®¡ç†å™¨]
        MCPClient[MCPå®¢æˆ·ç«¯]
    end
    
    CLI --> BaseAgent
    Interactive --> BaseAgent
    Config --> BaseAgent
    
    BaseAgent --> TraeAgent
    TraeAgent --> AgentExecution
    
    AgentExecution --> LLMClient
    LLMClient --> OpenAI
    LLMClient --> Anthropic
    LLMClient --> Google
    LLMClient --> Others
    
    AgentExecution --> ToolExecutor
    ToolExecutor --> DockerToolExecutor
    ToolExecutor --> Tools
    
    TraeAgent --> TrajectoryRecorder
    TraeAgent --> DockerManager
    TraeAgent --> MCPClient
```

### 2.2 æ ¸å¿ƒç»„ä»¶æŠ€æœ¯è¯¦è§£

#### 2.2.1 BaseAgent æŠ½è±¡åŸºç±»

BaseAgent æ˜¯æ‰€æœ‰ä»£ç†çš„åŸºç¡€ç±»ï¼Œå®šä¹‰äº†ä»£ç†çš„æ ¸å¿ƒç”Ÿå‘½å‘¨æœŸå’Œæ¥å£ï¼š

**å…³é”®æŠ€æœ¯ç‰¹ç‚¹ï¼š**

1. **ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š
   - `new_task()`: ä»»åŠ¡åˆå§‹åŒ–
   - `execute_task()`: ä»»åŠ¡æ‰§è¡Œ
   - `cleanup_mcp_clients()`: èµ„æºæ¸…ç†

2. **å¼‚æ­¥æ‰§è¡Œæ¨¡å‹**ï¼š
   ```python
   async def execute_task(self) -> AgentExecution:
       execution = AgentExecution(task=self._task, steps=[])
       while step_number <= self._max_steps:
           step = AgentStep(step_number=step_number, state=AgentStepState.THINKING)
           messages = await self._run_llm_step(step, messages, execution)
           await self._finalize_step(step, messages, execution)
   ```

3. **çŠ¶æ€æœºè®¾è®¡**ï¼š
   - `THINKING`: æ€è€ƒé˜¶æ®µ
   - `CALLING_TOOL`: å·¥å…·è°ƒç”¨é˜¶æ®µ
   - `REFLECTING`: åæ€é˜¶æ®µ
   - `COMPLETED`: å®Œæˆé˜¶æ®µ
   - `ERROR`: é”™è¯¯é˜¶æ®µ

#### 2.2.2 TraeAgent ä¸“ä¸šåŒ–å®ç°

TraeAgent ç»§æ‰¿è‡ª BaseAgentï¼Œä¸“é—¨é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼š

**æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ï¼š**

1. **MCPï¼ˆModel Context Protocolï¼‰é›†æˆ**ï¼š
   ```python
   async def discover_mcp_tools(self):
       if self.mcp_servers_config:
           for mcp_server_name, mcp_server_config in self.mcp_servers_config.items():
               mcp_client = MCPClient()
               await mcp_client.connect_and_discover(
                   mcp_server_name, mcp_server_config, self.mcp_tools
               )
   ```

2. **Git é›†æˆä¸è¡¥ä¸ç”Ÿæˆ**ï¼š
   ```python
   def get_git_diff(self) -> str:
       if not self.base_commit:
           stdout = subprocess.check_output(["git", "--no-pager", "diff"]).decode()
       else:
           stdout = subprocess.check_output(
               ["git", "--no-pager", "diff", self.base_commit, "HEAD"]
           ).decode()
   ```

3. **æ™ºèƒ½ä»»åŠ¡å®Œæˆæ£€æµ‹**ï¼š
   ```python
   def llm_indicates_task_completed(self, llm_response: LLMResponse) -> bool:
       return any(tool_call.name == "task_done" for tool_call in llm_response.tool_calls)
   ```

### 2.3 å·¥å…·ç³»ç»Ÿæ¶æ„

```mermaid
classDiagram
    class Tool {
        <<abstract>>
        +get_name() str
        +get_description() str
        +get_parameters() list[ToolParameter]
        +execute(arguments) ToolExecResult
        +json_definition() dict
        +get_input_schema() dict
    }
    
    class ToolExecutor {
        -tools: list[Tool]
        -tool_map: dict[str, Tool]
        +execute_tool_call(tool_call) ToolResult
        +parallel_tool_call(tool_calls) list[ToolResult]
        +sequential_tool_call(tool_calls) list[ToolResult]
    }
    
    class DockerToolExecutor {
        -original_executor: ToolExecutor
        -docker_manager: DockerManager
        -docker_tools: list[str]
        +execute_tool_call(tool_call) ToolResult
    }
    
    class BashTool {
        +execute(arguments) ToolExecResult
    }
    
    class EditTool {
        +execute(arguments) ToolExecResult
    }
    
    class SequentialThinkingTool {
        +execute(arguments) ToolExecResult
    }
    
    Tool <|-- BashTool
    Tool <|-- EditTool
    Tool <|-- SequentialThinkingTool
    ToolExecutor <|-- DockerToolExecutor
    ToolExecutor --> Tool
```

#### 2.3.1 å·¥å…·æ‰§è¡Œæœºåˆ¶

**å¹¶è¡Œæ‰§è¡Œæ”¯æŒï¼š**
```python
async def parallel_tool_call(self, tool_calls: list[ToolCall]) -> list[ToolResult]:
    return await asyncio.gather(*[self.execute_tool_call(call) for call in tool_calls])
```

**é¡ºåºæ‰§è¡Œæ”¯æŒï¼š**
```python
async def sequential_tool_call(self, tool_calls: list[ToolCall]) -> list[ToolResult]:
    return [await self.execute_tool_call(call) for call in tool_calls]
```

**Docker éš”ç¦»æ‰§è¡Œï¼š**
DockerToolExecutor æä¾›äº†å®¹å™¨åŒ–çš„å·¥å…·æ‰§è¡Œç¯å¢ƒï¼Œç¡®ä¿æ‰§è¡Œå®‰å…¨æ€§å’Œç¯å¢ƒä¸€è‡´æ€§ã€‚

### 2.4 LLM å®¢æˆ·ç«¯ç³»ç»Ÿ

```mermaid
graph LR
    subgraph "LLMå®¢æˆ·ç«¯æ¶æ„"
        LLMClient[LLMå®¢æˆ·ç«¯]
        BaseLLMClient[åŸºç¡€LLMå®¢æˆ·ç«¯]
        
        subgraph "å…·ä½“å®ç°"
            OpenAIClient[OpenAIå®¢æˆ·ç«¯]
            AnthropicClient[Anthropicå®¢æˆ·ç«¯]
            GoogleClient[Googleå®¢æˆ·ç«¯]
            OllamaClient[Ollamaå®¢æˆ·ç«¯]
        end
    end
    
    LLMClient --> BaseLLMClient
    BaseLLMClient <|-- OpenAIClient
    BaseLLMClient <|-- AnthropicClient
    BaseLLMClient <|-- GoogleClient
    BaseLLMClient <|-- OllamaClient
```

#### 2.4.1 ç»Ÿä¸€æ¥å£è®¾è®¡

**æ ¸å¿ƒæ¥å£æ–¹æ³•ï¼š**
```python
def chat(
    self,
    messages: list[LLMMessage],
    model_config: ModelConfig,
    tools: list[Tool] | None = None,
    reuse_history: bool = True,
) -> LLMResponse
```

**å·¥å…·è°ƒç”¨æ”¯æŒæ£€æµ‹ï¼š**
```python
def supports_tool_calling(self, model_config: ModelConfig) -> bool:
    return hasattr(self.client, "supports_tool_calling") and \
           self.client.supports_tool_calling(model_config)
```

## 3. é…ç½®ç³»ç»Ÿè¯¦è§£

### 3.1 é…ç½®æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "é…ç½®å±‚æ¬¡ç»“æ„"
        Config[ä¸»é…ç½®]
        
        subgraph "ä»£ç†é…ç½®"
            TraeAgentConfig[Traeä»£ç†é…ç½®]
            LakeviewConfig[Lakeviewé…ç½®]
        end
        
        subgraph "æ¨¡å‹é…ç½®"
            ModelConfig[æ¨¡å‹é…ç½®]
            ModelProvider[æ¨¡å‹æä¾›å•†é…ç½®]
        end
        
        subgraph "æœåŠ¡é…ç½®"
            MCPServerConfig[MCPæœåŠ¡å™¨é…ç½®]
        end
    end
    
    Config --> TraeAgentConfig
    Config --> LakeviewConfig
    Config --> ModelConfig
    Config --> MCPServerConfig
    
    TraeAgentConfig --> ModelConfig
    LakeviewConfig --> ModelConfig
    ModelConfig --> ModelProvider
```

### 3.2 é…ç½®ä¼˜å…ˆçº§æœºåˆ¶

ç³»ç»Ÿé‡‡ç”¨äº†**åˆ†å±‚é…ç½®ä¼˜å…ˆçº§**æœºåˆ¶ï¼š

```
å‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
```

**å®ç°ç»†èŠ‚ï¼š**
```python
def resolve_config_value(
    *,
    cli_value: int | str | float | None,
    config_value: int | str | float | None,
    env_var: str | None = None,
) -> int | str | float | None:
    if cli_value is not None:
        return cli_value
    
    if env_var and os.getenv(env_var):
        return os.getenv(env_var)
    
    if config_value is not None:
        return config_value
    
    return None
```

### 3.3 YAML é…ç½®ç¤ºä¾‹è¯¦è§£

```yaml
# ä»£ç†é…ç½®
agents:
  trae_agent:
    enable_lakeview: true
    model: trae_agent_model
    max_steps: 200
    tools:
      - bash
      - str_replace_based_edit_tool
      - sequentialthinking
      - task_done

# æ¨¡å‹æä¾›å•†é…ç½®
model_providers:
  anthropic:
    api_key: your_anthropic_api_key
    provider: anthropic
  openai:
    api_key: your_openai_api_key
    provider: openai
    base_url: https://api.openai.com/v1  # å¯é€‰

# æ¨¡å‹é…ç½®
models:
  trae_agent_model:
    model_provider: anthropic
    model: claude-sonnet-4-20250514
    max_tokens: 4096
    temperature: 0.5
    parallel_tool_calls: true

# MCPæœåŠ¡å™¨é…ç½®ï¼ˆå¯é€‰ï¼‰
mcp_servers:
  playwright:
    command: npx
    args:
      - "@playwright/mcp@0.0.27"
    timeout: 30
```

## 4. è½¨è¿¹è®°å½•ç³»ç»Ÿ

### 4.1 è½¨è¿¹è®°å½•æ¶æ„

```mermaid
sequenceDiagram
    participant Agent as TraeAgent
    participant Recorder as TrajectoryRecorder
    participant LLM as LLMClient
    participant Tool as ToolExecutor
    
    Agent->>Recorder: start_recording()
    
    loop æ‰§è¡Œæ­¥éª¤
        Agent->>LLM: chat()
        LLM-->>Agent: LLMResponse
        Agent->>Recorder: record_agent_step()
        
        Agent->>Tool: execute_tool_call()
        Tool-->>Agent: ToolResult
        Agent->>Recorder: record_agent_step()
    end
    
    Agent->>Recorder: finalize_recording()
```

### 4.2 è½¨è¿¹æ•°æ®ç»“æ„

**æ ¸å¿ƒè®°å½•å†…å®¹ï¼š**
- LLM äº¤äº’å†å²
- ä»£ç†æ‰§è¡Œæ­¥éª¤
- å·¥å…·ä½¿ç”¨æƒ…å†µ
- æ‰§è¡Œå…ƒæ•°æ®
- é”™è¯¯å’Œå¼‚å¸¸ä¿¡æ¯

**æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š**
```json
{
  "task": "Create a hello world Python script",
  "provider": "anthropic",
  "model": "claude-sonnet-4-20250514",
  "max_steps": 200,
  "steps": [
    {
      "step_number": 1,
      "state": "thinking",
      "llm_messages": [...],
      "llm_response": {...},
      "tool_calls": [...],
      "tool_results": [...],
      "timestamp": "2025-01-XX..."
    }
  ],
  "execution_time": 15.432,
  "success": true,
  "final_result": "Task completed successfully"
}
```

## 5. Docker é›†æˆç³»ç»Ÿ

### 5.1 Docker ç®¡ç†æ¶æ„

```mermaid
graph TB
    subgraph "Dockeré›†æˆç³»ç»Ÿ"
        DockerManager[Dockerç®¡ç†å™¨]
        DockerToolExecutor[Dockerå·¥å…·æ‰§è¡Œå™¨]
        
        subgraph "æ‰§è¡Œç¯å¢ƒ"
            Container[Dockerå®¹å™¨]
            Workspace[å·¥ä½œç©ºé—´æŒ‚è½½]
            Tools[å·¥å…·æŒ‚è½½]
        end
    end
    
    DockerManager --> Container
    DockerManager --> Workspace
    DockerManager --> Tools
    
    DockerToolExecutor --> DockerManager
    DockerToolExecutor --> Container
```

### 5.2 å®¹å™¨åŒ–æ‰§è¡Œç‰¹æ€§

**æ”¯æŒçš„ Docker æ¨¡å¼ï¼š**

1. **é•œåƒæ¨¡å¼**ï¼š`--docker-image python:3.11`
2. **å®¹å™¨æ¨¡å¼**ï¼š`--docker-container-id container_id`
3. **Dockerfile æ¨¡å¼**ï¼š`--dockerfile-path /path/to/Dockerfile`
4. **é•œåƒæ–‡ä»¶æ¨¡å¼**ï¼š`--docker-image-file image.tar`

**å·¥ä½œç©ºé—´æ˜ å°„ï¼š**
```python
self.docker_manager = DockerManager(
    image=docker_config.get("image"),
    workspace_dir=docker_config["workspace_dir"],
    tools_dir=tools_dir,
    interactive=is_interactive_mode,
)
```

## 6. Node.js å®ç°æŒ‡å—

ç°åœ¨æˆ‘å°†æä¾›å®Œæ•´çš„ Node.js å®ç°æ–¹æ¡ˆï¼š

### 6.1 é¡¹ç›®ç»“æ„è®¾è®¡

```
trae-agent-nodejs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ BaseAgent.ts
â”‚   â”‚   â”œâ”€â”€ TraeAgent.ts
â”‚   â”‚   â””â”€â”€ AgentBasics.ts
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ Tool.ts
â”‚   â”‚   â”‚   â””â”€â”€ ToolExecutor.ts
â”‚   â”‚   â”œâ”€â”€ BashTool.ts
â”‚   â”‚   â”œâ”€â”€ EditTool.ts
â”‚   â”‚   â””â”€â”€ SequentialThinkingTool.ts
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ LLMClient.ts
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIClient.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ AnthropicClient.ts
â”‚   â”‚   â”‚   â””â”€â”€ GoogleClient.ts
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ Config.ts
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ TrajectoryRecorder.ts
â”‚   â”‚   â”œâ”€â”€ DockerManager.ts
â”‚   â”‚   â””â”€â”€ MCPClient.ts
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ index.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

### 6.2 æ ¸å¿ƒç±»å‹å®šä¹‰

```typescript
// src/llm/types.ts
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  toolResult?: ToolResult;
}

export interface LLMResponse {
  content: string;
  toolCalls?: ToolCall[];
  usage?: TokenUsage;
}

export interface TokenUsage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

// src/tools/base/Tool.ts
export interface ToolCall {
  name: string;
  callId: string;
  arguments: Record<string, any>;
  id?: string;
}

export interface ToolResult {
  callId: string;
  name: string;
  success: boolean;
  result?: string;
  error?: string;
  id?: string;
}

export interface ToolExecResult {
  output?: string;
  error?: string;
  errorCode: number;
}

export interface ToolParameter {
  name: string;
  type: string | string[];
  description: string;
  enum?: string[];
  items?: Record<string, any>;
  required: boolean;
}
```

### 6.3 åŸºç¡€å·¥å…·ç±»å®ç°

```typescript
// src/tools/base/Tool.ts
export abstract class Tool {
  protected modelProvider?: string;

  constructor(modelProvider?: string) {
    this.modelProvider = modelProvider;
  }

  abstract getName(): string;
  abstract getDescription(): string;
  abstract getParameters(): ToolParameter[];
  abstract execute(arguments: Record<string, any>): Promise<ToolExecResult>;

  getModelProvider(): string | undefined {
    return this.modelProvider;
  }

  jsonDefinition(): Record<string, any> {
    return {
      name: this.getName(),
      description: this.getDescription(),
      parameters: this.getInputSchema(),
    };
  }

  getInputSchema(): Record<string, any> {
    const schema: Record<string, any> = {
      type: 'object',
    };

    const properties: Record<string, any> = {};
    const required: string[] = [];

    for (const param of this.getParameters()) {
      const paramSchema: Record<string, any> = {
        type: param.type,
        description: param.description,
      };

      // OpenAI strict mode support
      if (this.modelProvider === 'openai') {
        required.push(param.name);
        if (!param.required) {
          const currentType = paramSchema.type;
          if (typeof currentType === 'string') {
            paramSchema.type = [currentType, 'null'];
          } else if (Array.isArray(currentType) && !currentType.includes('null')) {
            paramSchema.type = [...currentType, 'null'];
          }
        }
      } else if (param.required) {
        required.push(param.name);
      }

      if (param.enum) {
        paramSchema.enum = param.enum;
      }

      if (param.items) {
        paramSchema.items = param.items;
      }

      if (this.modelProvider === 'openai' && param.type === 'object') {
        paramSchema.additionalProperties = false;
      }

      properties[param.name] = paramSchema;
    }

    schema.properties = properties;
    if (required.length > 0) {
      schema.required = required;
    }

    if (this.modelProvider === 'openai') {
      schema.additionalProperties = false;
    }

    return schema;
  }

  async close(): Promise<void> {
    // Default implementation - can be overridden
  }
}
```

### 6.4 å·¥å…·æ‰§è¡Œå™¨å®ç°

```typescript
// src/tools/base/ToolExecutor.ts
export class ToolExecutor {
  private tools: Tool[];
  private toolMap?: Map<string, Tool>;

  constructor(tools: Tool[]) {
    this.tools = tools;
  }

  async closeTools(): Promise<void[]> {
    const tasks = this.tools.map(tool => tool.close());
    return Promise.all(tasks);
  }

  private normalizeToolName(name: string): string {
    return name.toLowerCase().replace(/_/g, '');
  }

  private getToolMap(): Map<string, Tool> {
    if (!this.toolMap) {
      this.toolMap = new Map();
      for (const tool of this.tools) {
        const normalizedName = this.normalizeToolName(tool.getName());
        this.toolMap.set(normalizedName, tool);
      }
    }
    return this.toolMap;
  }

  async executeToolCall(toolCall: ToolCall): Promise<ToolResult> {
    const normalizedName = this.normalizeToolName(toolCall.name);
    const toolMap = this.getToolMap();
    
    if (!toolMap.has(normalizedName)) {
      return {
        name: toolCall.name,
        success: false,
        error: `Tool '${toolCall.name}' not found. Available tools: ${this.tools.map(t => t.getName()).join(', ')}`,
        callId: toolCall.callId,
        id: toolCall.id,
      };
    }

    const tool = toolMap.get(normalizedName)!;

    try {
      const execResult = await tool.execute(toolCall.arguments);
      return {
        name: toolCall.name,
        success: execResult.errorCode === 0,
        result: execResult.output,
        error: execResult.error,
        callId: toolCall.callId,
        id: toolCall.id,
      };
    } catch (error) {
      return {
        name: toolCall.name,
        success: false,
        error: `Error executing tool '${toolCall.name}': ${error instanceof Error ? error.message : String(error)}`,
        callId: toolCall.callId,
        id: toolCall.id,
      };
    }
  }

  async parallelToolCall(toolCalls: ToolCall[]): Promise<ToolResult[]> {
    const tasks = toolCalls.map(call => this.executeToolCall(call));
    return Promise.all(tasks);
  }

  async sequentialToolCall(toolCalls: ToolCall[]): Promise<ToolResult[]> {
    const results: ToolResult[] = [];
    for (const call of toolCalls) {
      const result = await this.executeToolCall(call);
      results.push(result);
    }
    return results;
  }
}
```

### 6.5 LLM å®¢æˆ·ç«¯å®ç°

```typescript
// src/llm/LLMClient.ts
export enum LLMProvider {
  OPENAI = 'openai',
  ANTHROPIC = 'anthropic',
  AZURE = 'azure',
  OLLAMA = 'ollama',
  OPENROUTER = 'openrouter',
  DOUBAO = 'doubao',
  GOOGLE = 'google',
}

export class LLMClient {
  private provider: LLMProvider;
  private modelConfig: ModelConfig;
  private client: BaseLLMClient;

  constructor(modelConfig: ModelConfig) {
    this.provider = modelConfig.modelProvider.provider as LLMProvider;
    this.modelConfig = modelConfig;

    switch (this.provider) {
      case LLMProvider.OPENAI:
        this.client = new OpenAIClient(modelConfig);
        break;
      case LLMProvider.ANTHROPIC:
        this.client = new AnthropicClient(modelConfig);
        break;
      case LLMProvider.GOOGLE:
        this.client = new GoogleClient(modelConfig);
        break;
      // ... å…¶ä»–æä¾›å•†
      default:
        throw new Error(`Unsupported provider: ${this.provider}`);
    }
  }

  setTrajectoryRecorder(recorder?: TrajectoryRecorder): void {
    this.client.setTrajectoryRecorder(recorder);
  }

  setChatHistory(messages: LLMMessage[]): void {
    this.client.setChatHistory(messages);
  }

  chat(
    messages: LLMMessage[],
    modelConfig: ModelConfig,
    tools?: Tool[],
    reuseHistory: boolean = true
  ): Promise<LLMResponse> {
    return this.client.chat(messages, modelConfig, tools, reuseHistory);
  }

  supportsToolCalling(modelConfig: ModelConfig): boolean {
    return this.client.supportsToolCalling && this.client.supportsToolCalling(modelConfig);
  }
}
```

### 6.6 åŸºç¡€ä»£ç†å®ç°

```typescript
// src/agent/BaseAgent.ts
export abstract class BaseAgent {
  protected llmClient: LLMClient;
  protected modelConfig: ModelConfig;
  protected maxSteps: number;
  protected initialMessages: LLMMessage[] = [];
  protected task: string = '';
  protected tools: Tool[];
  protected toolCaller: ToolExecutor;
  protected trajectoryRecorder?: TrajectoryRecorder;

  constructor(agentConfig: AgentConfig) {
    this.llmClient = new LLMClient(agentConfig.model);
    this.modelConfig = agentConfig.model;
    this.maxSteps = agentConfig.maxSteps;
    
    // åˆå§‹åŒ–å·¥å…·
    this.tools = agentConfig.tools.map(toolName => {
      const ToolClass = toolsRegistry[toolName];
      return new ToolClass(this.modelConfig.modelProvider.provider);
    });
    
    this.toolCaller = new ToolExecutor(this.tools);
  }

  setTrajectoryRecorder(recorder?: TrajectoryRecorder): void {
    this.trajectoryRecorder = recorder;
    this.llmClient.setTrajectoryRecorder(recorder);
  }

  abstract newTask(
    task: string,
    extraArgs?: Record<string, string>,
    toolNames?: string[]
  ): void;

  async executeTask(): Promise<AgentExecution> {
    const startTime = Date.now();
    const execution: AgentExecution = {
      task: this.task,
      steps: [],
      agentState: AgentState.RUNNING,
      success: false,
      executionTime: 0,
    };

    try {
      if (this.trajectoryRecorder) {
        this.trajectoryRecorder.startRecording({
          task: this.task,
          provider: this.llmClient.provider,
          model: this.modelConfig.model,
          maxSteps: this.maxSteps,
        });
      }

      let messages = [...this.initialMessages];
      let stepNumber = 1;

      while (stepNumber <= this.maxSteps) {
        const step: AgentStep = {
          stepNumber,
          state: AgentStepState.THINKING,
        };

        try {
          messages = await this.runLLMStep(step, messages, execution);
          await this.finalizeStep(step, messages, execution);
          
          if (execution.agentState === AgentState.COMPLETED) {
            break;
          }
          
          stepNumber++;
        } catch (error) {
          execution.agentState = AgentState.ERROR;
          step.state = AgentStepState.ERROR;
          step.error = error instanceof Error ? error.message : String(error);
          await this.finalizeStep(step, messages, execution);
          break;
        }
      }

      if (stepNumber > this.maxSteps && !execution.success) {
        execution.finalResult = 'Task execution exceeded maximum steps without completion.';
        execution.agentState = AgentState.ERROR;
      }

    } catch (error) {
      execution.finalResult = `Agent execution failed: ${error instanceof Error ? error.message : String(error)}`;
    } finally {
      await this.closeTools();
    }

    execution.executionTime = Date.now() - startTime;

    if (this.trajectoryRecorder) {
      this.trajectoryRecorder.finalizeRecording({
        success: execution.success,
        finalResult: execution.finalResult,
      });
    }

    return execution;
  }

  private async runLLMStep(
    step: AgentStep,
    messages: LLMMessage[],
    execution: AgentExecution
  ): Promise<LLMMessage[]> {
    // è·å– LLM å“åº”
    step.state = AgentStepState.THINKING;
    const llmResponse = await this.llmClient.chat(messages, this.modelConfig, this.tools);
    step.llmResponse = llmResponse;

    // æ›´æ–° token ä½¿ç”¨æƒ…å†µ
    this.updateLLMUsage(llmResponse, execution);

    if (this.llmIndicatesTaskCompleted(llmResponse)) {
      if (this.isTaskCompleted(llmResponse)) {
        execution.agentState = AgentState.COMPLETED;
        execution.finalResult = llmResponse.content;
        execution.success = true;
        return messages;
      } else {
        execution.agentState = AgentState.RUNNING;
        return [{
          role: 'user',
          content: this.taskIncompleteMessage(),
        }];
      }
    } else {
      return await this.toolCallHandler(llmResponse.toolCalls, step);
    }
  }

  private async toolCallHandler(
    toolCalls: ToolCall[] | undefined,
    step: AgentStep
  ): Promise<LLMMessage[]> {
    const messages: LLMMessage[] = [];
    
    if (!toolCalls || toolCalls.length === 0) {
      messages.push({
        role: 'user',
        content: 'It seems that you have not completed the task.',
      });
      return messages;
    }

    step.state = AgentStepState.CALLING_TOOL;
    step.toolCalls = toolCalls;

    let toolResults: ToolResult[];
    if (this.modelConfig.parallelToolCalls) {
      toolResults = await this.toolCaller.parallelToolCall(toolCalls);
    } else {
      toolResults = await this.toolCaller.sequentialToolCall(toolCalls);
    }
    
    step.toolResults = toolResults;

    for (const toolResult of toolResults) {
      messages.push({
        role: 'user',
        toolResult,
      });
    }

    const reflection = this.reflectOnResult(toolResults);
    if (reflection) {
      step.state = AgentStepState.REFLECTING;
      step.reflection = reflection;
      messages.push({
        role: 'assistant',
        content: reflection,
      });
    }

    return messages;
  }

  protected reflectOnResult(toolResults: ToolResult[]): string | undefined {
    if (toolResults.length === 0) {
      return undefined;
    }

    const failedResults = toolResults.filter(result => !result.success);
    if (failedResults.length === 0) {
      return undefined;
    }

    return failedResults
      .map(result => `The tool execution failed with error: ${result.error}. Consider trying a different approach or fixing the parameters.`)
      .join('\n');
  }

  protected llmIndicatesTaskCompleted(llmResponse: LLMResponse): boolean {
    const completionIndicators = [
      'task completed',
      'task finished',
      'done',
      'completed successfully',
      'finished successfully',
    ];

    const responseContent = llmResponse.content.toLowerCase();
    return completionIndicators.some(indicator => responseContent.includes(indicator));
  }

  protected isTaskCompleted(llmResponse: LLMResponse): boolean {
    return true; // é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™
  }

  protected taskIncompleteMessage(): string {
    return 'The task is incomplete. Please try again.';
  }

  private async finalizeStep(
    step: AgentStep,
    messages: LLMMessage[],
    execution: AgentExecution
  ): Promise<void> {
    step.state = AgentStepState.COMPLETED;
    this.recordHandler(step, messages);
    execution.steps.push(step);
  }

  private recordHandler(step: AgentStep, messages: LLMMessage[]): void {
    if (this.trajectoryRecorder) {
      this.trajectoryRecorder.recordAgentStep({
        stepNumber: step.stepNumber,
        state: step.state,
        llmMessages: messages,
        llmResponse: step.llmResponse,
        toolCalls: step.toolCalls,
        toolResults: step.toolResults,
        reflection: step.reflection,
        error: step.error,
      });
    }
  }

  private updateLLMUsage(llmResponse: LLMResponse, execution: AgentExecution): void {
    if (!llmResponse.usage) {
      return;
    }

    if (!execution.totalTokens) {
      execution.totalTokens = llmResponse.usage;
    } else {
      execution.totalTokens.promptTokens += llmResponse.usage.promptTokens;
      execution.totalTokens.completionTokens += llmResponse.usage.completionTokens;
      execution.totalTokens.totalTokens += llmResponse.usage.totalTokens;
    }
  }

  private async closeTools(): Promise<void> {
    if (this.toolCaller) {
      await this.toolCaller.closeTools();
    }
  }
}
```

### 6.7 TraeAgent ä¸“ä¸šåŒ–å®ç°

```typescript
// src/agent/TraeAgent.ts
export class TraeAgent extends BaseAgent {
  private projectPath: string = '';
  private baseCommit?: string;
  private mustPatch: string = 'false';
  private patchPath?: string;
  private mcpServersConfig?: Record<string, MCPServerConfig>;
  private allowMcpServers: string[] = [];
  private mcpTools: Tool[] = [];
  private mcpClients: MCPClient[] = [];

  constructor(traeAgentConfig: TraeAgentConfig) {
    super(traeAgentConfig);
    this.mcpServersConfig = traeAgentConfig.mcpServersConfig;
    this.allowMcpServers = traeAgentConfig.allowMcpServers || [];
  }

  async initializeMcp(): Promise<void> {
    await this.discoverMcpTools();
    
    if (this.mcpTools.length > 0) {
      this.tools.push(...this.mcpTools);
      // é‡æ–°åˆ›å»ºå·¥å…·æ‰§è¡Œå™¨
      this.toolCaller = new ToolExecutor(this.tools);
    }
  }

  private async discoverMcpTools(): Promise<void> {
    if (!this.mcpServersConfig) {
      return;
    }

    for (const [serverName, serverConfig] of Object.entries(this.mcpServersConfig)) {
      if (this.allowMcpServers.length > 0 && !this.allowMcpServers.includes(serverName)) {
        continue;
      }

      const mcpClient = new MCPClient();
      try {
        await mcpClient.connectAndDiscover(
          serverName,
          serverConfig,
          this.mcpTools,
          this.llmClient.provider
        );
        this.mcpClients.push(mcpClient);
      } catch (error) {
        // æ¸…ç†å¤±è´¥çš„å®¢æˆ·ç«¯
        try {
          await mcpClient.cleanup(serverName);
        } catch {
          // å¿½ç•¥æ¸…ç†é”™è¯¯
        }
        continue;
      }
    }
  }

  newTask(
    task: string,
    extraArgs?: Record<string, string>,
    toolNames?: string[]
  ): void {
    this.task = task;

    if (!extraArgs) {
      throw new Error('Project path and issue information are required.');
    }
    
    if (!extraArgs.projectPath) {
      throw new Error('Project path is required');
    }

    this.projectPath = extraArgs.projectPath;
    
    let userMessage = `[Project root path]:\n${this.projectPath}\n\n`;
    
    if (extraArgs.issue) {
      userMessage += `[Problem statement]: We're currently solving the following issue within our repository. Here's the issue text:\n${extraArgs.issue}\n`;
    }

    // è®¾ç½®å¯é€‰å±æ€§
    if (extraArgs.baseCommit) this.baseCommit = extraArgs.baseCommit;
    if (extraArgs.mustPatch) this.mustPatch = extraArgs.mustPatch;
    if (extraArgs.patchPath) this.patchPath = extraArgs.patchPath;

    this.initialMessages = [
      { role: 'system', content: this.getSystemPrompt() },
      { role: 'user', content: userMessage },
    ];

    // å¼€å§‹è½¨è¿¹è®°å½•
    if (this.trajectoryRecorder) {
      this.trajectoryRecorder.startRecording({
        task,
        provider: this.llmClient.provider,
        model: this.modelConfig.model,
        maxSteps: this.maxSteps,
      });
    }
  }

  async executeTask(): Promise<AgentExecution> {
    const execution = await super.executeTask();

    // ç”Ÿæˆè¡¥ä¸æ–‡ä»¶
    if (this.patchPath) {
      const gitDiff = this.getGitDiff();
      await fs.writeFile(this.patchPath, gitDiff, 'utf8');
    }

    return execution;
  }

  private getSystemPrompt(): string {
    // è¿”å› Trae Agent çš„ç³»ç»Ÿæç¤ºè¯
    return `You are Trae Agent, an AI assistant specialized in software engineering tasks...`;
  }

  protected llmIndicatesTaskCompleted(llmResponse: LLMResponse): boolean {
    if (!llmResponse.toolCalls) {
      return false;
    }
    return llmResponse.toolCalls.some(toolCall => toolCall.name === 'task_done');
  }

  protected isTaskCompleted(llmResponse: LLMResponse): boolean {
    if (this.mustPatch === 'true') {
      const modelPatch = this.getGitDiff();
      const patch = this.removePatchesToTests(modelPatch);
      if (!patch.trim()) {
        return false;
      }
    }
    return true;
  }

  protected taskIncompleteMessage(): string {
    return 'ERROR! Your Patch is empty. Please provide a patch that fixes the problem.';
  }

  private getGitDiff(): string {
    try {
      const { execSync } = require('child_process');
      const originalCwd = process.cwd();
      
      if (!fs.existsSync(this.projectPath)) {
        return '';
      }
      
      process.chdir(this.projectPath);
      
      try {
        let stdout: string;
        if (!this.baseCommit) {
          stdout = execSync('git --no-pager diff', { encoding: 'utf8' });
        } else {
          stdout = execSync(`git --no-pager diff ${this.baseCommit} HEAD`, { encoding: 'utf8' });
        }
        return stdout;
      } finally {
        process.chdir(originalCwd);
      }
    } catch {
      return '';
    }
  }

  private removePatchesToTests(modelPatch: string): string {
    const lines = modelPatch.split('\n');
    const filteredLines: string[] = [];
    const testPatterns = ['/test/', '/tests/', '/testing/', 'test_', 'tox.ini'];
    let isTests = false;

    for (const line of lines) {
      if (line.startsWith('diff --git a/')) {
        const targetPath = line.split(' ').pop() || '';
        isTests = targetPath.startsWith('b/') && 
                  testPatterns.some(pattern => targetPath.includes(pattern));
      }

      if (!isTests) {
        filteredLines.push(line);
      }
    }

    return filteredLines.join('\n');
  }

  async cleanup(): Promise<void> {
    for (const client of this.mcpClients) {
      try {
        await client.cleanup('cleanup');
      } catch {
        // å¿½ç•¥æ¸…ç†é”™è¯¯
      }
    }
    this.mcpClients = [];
  }
}
```

### 6.8 é…ç½®ç³»ç»Ÿå®ç°

```typescript
// src/config/Config.ts
export class Config {
  lakeview?: LakeviewConfig;
  modelProviders?: Record<string, ModelProvider>;
  models?: Record<string, ModelConfig>;
  traeAgent?: TraeAgentConfig;

  static async create(options: {
    configFile?: string;
    configString?: string;
  }): Promise<Config> {
    const { configFile, configString } = options;
    
    if (configFile && configString) {
      throw new ConfigError('Only one of configFile or configString should be provided');
    }

    let yamlConfig: any;
    
    try {
      if (configFile) {
        if (configFile.endsWith('.json')) {
          return this.createFromLegacyConfig({ configFile });
        }
        const content = await fs.readFile(configFile, 'utf8');
        yamlConfig = yaml.parse(content);
      } else if (configString) {
        yamlConfig = yaml.parse(configString);
      } else {
        throw new ConfigError('No config file or config string provided');
      }
    } catch (error) {
      throw new ConfigError(`Error parsing YAML config: ${error}`);
    }

    const config = new Config();

    // è§£ææ¨¡å‹æä¾›å•†
    const modelProviders = yamlConfig.model_providers;
    if (!modelProviders || Object.keys(modelProviders).length === 0) {
      throw new ConfigError('No model providers provided');
    }

    config.modelProviders = {};
    for (const [name, providerConfig] of Object.entries(modelProviders)) {
      config.modelProviders[name] = new ModelProvider(providerConfig as any);
    }

    // è§£ææ¨¡å‹é…ç½®
    const models = yamlConfig.models;
    if (!models || Object.keys(models).length === 0) {
      throw new ConfigError('No models provided');
    }

    config.models = {};
    for (const [modelName, modelConfig] of Object.entries(models as any)) {
      const providerName = modelConfig.model_provider;
      if (!config.modelProviders[providerName]) {
        throw new ConfigError(`Model provider ${providerName} not found`);
      }
      
      config.models[modelName] = new ModelConfig({
        ...modelConfig,
        modelProvider: config.modelProviders[providerName],
      });
    }

    // è§£æ MCP æœåŠ¡å™¨é…ç½®
    const mcpServersConfig: Record<string, MCPServerConfig> = {};
    const mcpServers = yamlConfig.mcp_servers || {};
    for (const [name, serverConfig] of Object.entries(mcpServers)) {
      mcpServersConfig[name] = new MCPServerConfig(serverConfig as any);
    }

    const allowMcpServers = yamlConfig.allow_mcp_servers || [];

    // è§£æä»£ç†é…ç½®
    const agents = yamlConfig.agents;
    if (!agents || Object.keys(agents).length === 0) {
      throw new ConfigError('No agent configs provided');
    }

    for (const [agentName, agentConfig] of Object.entries(agents as any)) {
      const modelName = agentConfig.model;
      if (!modelName) {
        throw new ConfigError(`No model provided for ${agentName}`);
      }
      
      const model = config.models[modelName];
      if (!model) {
        throw new ConfigError(`Model ${modelName} not found`);
      }

      switch (agentName) {
        case 'trae_agent':
          config.traeAgent = new TraeAgentConfig({
            ...agentConfig,
            model,
            mcpServersConfig,
            allowMcpServers,
          });
          break;
        default:
          throw new ConfigError(`Unknown agent: ${agentName}`);
      }
    }

    return config;
  }

  resolveConfigValues(options: {
    provider?: string;
    model?: string;
    modelBaseUrl?: string;
    apiKey?: string;
    maxSteps?: number;
  }): Config {
    if (this.traeAgent) {
      this.traeAgent.resolveConfigValues({
        maxSteps: options.maxSteps,
      });
      
      this.traeAgent.model.resolveConfigValues({
        modelProviders: this.modelProviders,
        provider: options.provider,
        model: options.model,
        modelBaseUrl: options.modelBaseUrl,
        apiKey: options.apiKey,
      });
    }
    
    return this;
  }
}

function resolveConfigValue(options: {
  cliValue?: any;
  configValue?: any;
  envVar?: string;
}): any {
  const { cliValue, configValue, envVar } = options;
  
  if (cliValue !== undefined) {
    return cliValue;
  }
  
  if (envVar && process.env[envVar]) {
    return process.env[envVar];
  }
  
  if (configValue !== undefined) {
    return configValue;
  }
  
  return undefined;
}
```

### 6.9 CLI æ¥å£å®ç°

```typescript
// src/cli/index.ts
import { Command } from 'commander';
import { Config } from '../config/Config';
import { TraeAgent } from '../agent/TraeAgent';
import { TrajectoryRecorder } from '../utils/TrajectoryRecorder';

const program = new Command();

program
  .name('trae-cli')
  .description('Trae Agent - LLM-based agent for software engineering tasks')
  .version('1.0.0');

program
  .command('run')
  .description('Execute a task')
  .argument('<task>', 'Task description')
  .option('--config <file>', 'Configuration file path', 'trae_config.yaml')
  .option('--provider <provider>', 'LLM provider')
  .option('--model <model>', 'Model name')
  .option('--max-steps <steps>', 'Maximum steps', parseInt)
  .option('--working-dir <dir>', 'Working directory', process.cwd())
  .option('--trajectory-file <file>', 'Trajectory output file')
  .option('--must-patch', 'Force patch generation')
  .action(async (task: string, options) => {
    try {
      // åŠ è½½é…ç½®
      const config = await Config.create({ configFile: options.config });
      
      // è§£æé…ç½®å€¼
      config.resolveConfigValues({
        provider: options.provider,
        model: options.model,
        maxSteps: options.maxSteps,
      });

      if (!config.traeAgent) {
        throw new Error('No trae_agent configuration found');
      }

      // åˆ›å»ºä»£ç†
      const agent = new TraeAgent(config.traeAgent);
      
      // åˆå§‹åŒ– MCP
      await agent.initializeMcp();

      // è®¾ç½®è½¨è¿¹è®°å½•å™¨
      if (options.trajectoryFile) {
        const recorder = new TrajectoryRecorder(options.trajectoryFile);
        agent.setTrajectoryRecorder(recorder);
      }

      // åˆ›å»ºä»»åŠ¡
      const extraArgs: Record<string, string> = {
        projectPath: options.workingDir,
        issue: task,
      };
      
      if (options.mustPatch) {
        extraArgs.mustPatch = 'true';
      }

      agent.newTask(task, extraArgs);

      // æ‰§è¡Œä»»åŠ¡
      console.log(`Executing task: ${task}`);
      const execution = await agent.executeTask();

      // è¾“å‡ºç»“æœ
      if (execution.success) {
        console.log('âœ… Task completed successfully!');
        console.log(`Final result: ${execution.finalResult}`);
      } else {
        console.log('âŒ Task failed');
        console.log(`Error: ${execution.finalResult}`);
      }

      console.log(`Execution time: ${execution.executionTime}ms`);
      console.log(`Steps completed: ${execution.steps.length}`);
      
      if (execution.totalTokens) {
        console.log(`Tokens used: ${execution.totalTokens.totalTokens}`);
      }

      // æ¸…ç†èµ„æº
      await agent.cleanup();

    } catch (error) {
      console.error('Error:', error instanceof Error ? error.message : String(error));
      process.exit(1);
    }
  });

program
  .command('interactive')
  .description('Start interactive mode')
  .option('--config <file>', 'Configuration file path', 'trae_config.yaml')
  .action(async (options) => {
    // äº¤äº’æ¨¡å¼å®ç°
    console.log('Interactive mode not yet implemented');
  });

program
  .command('show-config')
  .description('Show current configuration')
  .option('--config <file>', 'Configuration file path', 'trae_config.yaml')
  .action(async (options) => {
    try {
      const config = await Config.create({ configFile: options.config });
      console.log(JSON.stringify(config, null, 2));
    } catch (error) {
      console.error('Error:', error instanceof Error ? error.message : String(error));
      process.exit(1);
    }
  });

program.parse();
```

### 6.10 Package.json é…ç½®

```json
{
  "name": "trae-agent-nodejs",
  "version": "1.0.0",
  "description": "Trae Agentçš„Node.jså®ç° - åŸºäºLLMçš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä»£ç†",
  "main": "dist/index.js",
  "bin": {
    "trae-cli": "dist/cli/index.js"
  },
  "scripts": {
    "build": "tsc",
    "dev": "ts-node src/cli/index.ts",
    "start": "node dist/cli/index.js",
    "test": "jest",
    "lint": "eslint src/**/*.ts",
    "format": "prettier --write src/**/*.ts"
  },
  "dependencies": {
    "openai": "^4.0.0",
    "@anthropic-ai/sdk": "^0.24.0",
    "@google/generative-ai": "^0.15.0",
    "commander": "^11.0.0",
    "yaml": "^2.3.0",
    "dotenv": "^16.0.0",
    "chalk": "^5.0.0",
    "ora": "^7.0.0",
    "inquirer": "^9.0.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "@types/jest": "^29.0.0",
    "typescript": "^5.0.0",
    "ts-node": "^10.0.0",
    "jest": "^29.0.0",
    "eslint": "^8.0.0",
    "@typescript-eslint/eslint-plugin": "^6.0.0",
    "@typescript-eslint/parser": "^6.0.0",
    "prettier": "^3.0.0"
  },
  "keywords": [
    "ai",
    "llm",
    "agent",
    "software-engineering",
    "automation",
    "typescript",
    "nodejs"
  ],
  "author": "Your Name",
  "license": "MIT"
}
```

## 7. éƒ¨ç½²ä¸ä½¿ç”¨æŒ‡å—

### 7.1 å®‰è£…ä¸é…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd trae-agent-nodejs

# å®‰è£…ä¾èµ–
npm install

# æ„å»ºé¡¹ç›®
npm run build

# åˆ›å»ºé…ç½®æ–‡ä»¶
cp trae_config.yaml.example trae_config.yaml
```

### 7.2 é…ç½®ç¤ºä¾‹

```yaml
# trae_config.yaml
agents:
  trae_agent:
    enable_lakeview: true
    model: trae_agent_model
    max_steps: 200
    tools:
      - bash
      - str_replace_based_edit_tool
      - sequentialthinking
      - task_done

model_providers:
  openai:
    api_key: your_openai_api_key
    provider: openai
  anthropic:
    api_key: your_anthropic_api_key
    provider: anthropic

models:
  trae_agent_model:
    model_provider: anthropic
    model: claude-3-5-sonnet-20241022
    max_tokens: 4096
    temperature: 0.5
    parallel_tool_calls: true
```

### 7.3 ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬ä»»åŠ¡æ‰§è¡Œ
trae-cli run "Create a hello world Python script"

# æŒ‡å®šå·¥ä½œç›®å½•
trae-cli run "Add unit tests for the utils module" --working-dir ./my-project

# ä½¿ç”¨ç‰¹å®šæ¨¡å‹
trae-cli run "Fix the bug in main.py" --provider openai --model gpt-4

# ä¿å­˜æ‰§è¡Œè½¨è¿¹
trae-cli run "Optimize the database queries" --trajectory-file debug.json

# å¼ºåˆ¶ç”Ÿæˆè¡¥ä¸
trae-cli run "Update API endpoints" --must-patch

# æŸ¥çœ‹é…ç½®
trae-cli show-config

# äº¤äº’æ¨¡å¼
trae-cli interactive
```

## 8. æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”ä¸ä¼˜åŠ¿

### 8.1 Python vs Node.js å®ç°å¯¹æ¯”

| ç‰¹æ€§ | Python ç‰ˆæœ¬ | Node.js ç‰ˆæœ¬ | ä¼˜åŠ¿åˆ†æ |
|------|-------------|-------------|----------|
| **å¼‚æ­¥å¤„ç†** | asyncio | åŸç”Ÿ async/await | Node.js å¼‚æ­¥æ€§èƒ½æ›´ä¼˜ |
| **ç±»å‹å®‰å…¨** | Python ç±»å‹æç¤º | TypeScript å¼ºç±»å‹ | TypeScript ç¼–è¯‘æ—¶æ£€æŸ¥æ›´ä¸¥æ ¼ |
| **ç”Ÿæ€ç³»ç»Ÿ** | ä¸°å¯Œçš„ AI/ML åº“ | ä¸°å¯Œçš„ Web ç”Ÿæ€ | å„æœ‰ä¼˜åŠ¿ |
| **æ€§èƒ½** | è§£é‡Šæ‰§è¡Œ | V8 å¼•æ“ä¼˜åŒ– | Node.js æ‰§è¡Œæ•ˆç‡æ›´é«˜ |
| **éƒ¨ç½²** | éœ€è¦ Python ç¯å¢ƒ | å•ä¸€å¯æ‰§è¡Œæ–‡ä»¶ | Node.js éƒ¨ç½²æ›´ä¾¿æ· |
| **å†…å­˜ä½¿ç”¨** | è¾ƒé«˜ | ç›¸å¯¹è¾ƒä½ | Node.js å†…å­˜æ•ˆç‡æ›´å¥½ |

### 8.2 æ¶æ„ä¼˜åŠ¿

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶éƒ½æœ‰æ¸…æ™°çš„èŒè´£è¾¹ç•Œ
2. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰å·¥å…·å’Œ LLM æä¾›å•†
3. **å®¹é”™æ€§**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œèµ„æºæ¸…ç†æœºåˆ¶
4. **å¯è§‚æµ‹æ€§**ï¼šè¯¦ç»†çš„è½¨è¿¹è®°å½•å’Œæ—¥å¿—ç³»ç»Ÿ
5. **é…ç½®çµæ´»æ€§**ï¼šæ”¯æŒå¤šå±‚çº§é…ç½®ä¼˜å…ˆçº§

## 9. é«˜çº§ç‰¹æ€§ä¸æœ€ä½³å®è·µ

### 9.1 è‡ªå®šä¹‰å·¥å…·å¼€å‘

```typescript
// ç¤ºä¾‹ï¼šåˆ›å»ºè‡ªå®šä¹‰æ•°æ®åº“å·¥å…·
export class DatabaseTool extends Tool {
  getName(): string {
    return 'database_query';
  }

  getDescription(): string {
    return 'Execute database queries and return results';
  }

  getParameters(): ToolParameter[] {
    return [
      {
        name: 'query',
        type: 'string',
        description: 'SQL query to execute',
        required: true,
      },
      {
        name: 'database',
        type: 'string',
        description: 'Database name',
        required: false,
      },
    ];
  }

  async execute(arguments: Record<string, any>): Promise<ToolExecResult> {
    try {
      const { query, database } = arguments;
      // æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
      const result = await this.executeQuery(query, database);
      
      return {
        output: JSON.stringify(result),
        errorCode: 0,
      };
    } catch (error) {
      return {
        error: error instanceof Error ? error.message : String(error),
        errorCode: 1,
      };
    }
  }

  private async executeQuery(query: string, database?: string): Promise<any> {
    // å®é™…çš„æ•°æ®åº“æŸ¥è¯¢å®ç°
    // ...
  }
}
```

### 9.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

1. **å·¥å…·å¹¶è¡Œæ‰§è¡Œ**ï¼šå¯ç”¨ `parallel_tool_calls` é…ç½®
2. **è¿æ¥æ± ç®¡ç†**ï¼šä¸º LLM å®¢æˆ·ç«¯å®ç°è¿æ¥å¤ç”¨
3. **ç¼“å­˜æœºåˆ¶**ï¼šå¯¹é‡å¤æŸ¥è¯¢ç»“æœè¿›è¡Œç¼“å­˜
4. **èµ„æºé™åˆ¶**ï¼šè®¾ç½®åˆç†çš„ `max_steps` å’Œ `max_tokens`
5. **å¼‚æ­¥ä¼˜åŒ–**ï¼šå……åˆ†åˆ©ç”¨ Node.js çš„å¼‚æ­¥ç‰¹æ€§

### 9.3 å®‰å…¨è€ƒè™‘

1. **è¾“å…¥éªŒè¯**ï¼šä¸¥æ ¼éªŒè¯æ‰€æœ‰ç”¨æˆ·è¾“å…¥
2. **æƒé™æ§åˆ¶**ï¼šé™åˆ¶å·¥å…·çš„æ–‡ä»¶ç³»ç»Ÿè®¿é—®æƒé™
3. **æ²™ç®±æ‰§è¡Œ**ï¼šä½¿ç”¨ Docker å®¹å™¨éš”ç¦»æ‰§è¡Œç¯å¢ƒ
4. **API å¯†é’¥ç®¡ç†**ï¼šå®‰å…¨å­˜å‚¨å’Œè½®æ¢ API å¯†é’¥
5. **å®¡è®¡æ—¥å¿—**ï¼šè®°å½•æ‰€æœ‰æ•æ„Ÿæ“ä½œ

## 10. æ€»ç»“

Trae Agent ä»£è¡¨äº† AI ä»£ç†ç³»ç»Ÿè®¾è®¡çš„å…ˆè¿›ç†å¿µï¼Œé€šè¿‡å…¶**é€æ˜ã€æ¨¡å—åŒ–ã€ç ”ç©¶å‹å¥½**çš„æ¶æ„è®¾è®¡ï¼Œä¸ºè½¯ä»¶å·¥ç¨‹è‡ªåŠ¨åŒ–æä¾›äº†å¼ºå¤§çš„åŸºç¡€å¹³å°ã€‚æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†å…¶æ ¸å¿ƒæŠ€æœ¯å®ç°ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„ Node.js ç§»æ¤æ–¹æ¡ˆã€‚

### 10.1 æ ¸å¿ƒä»·å€¼

1. **æŠ€æœ¯åˆ›æ–°**ï¼šMCP åè®®é›†æˆã€æ™ºèƒ½è½¨è¿¹è®°å½•ã€å¤š LLM æ”¯æŒ
2. **æ¶æ„ä¼˜åŠ¿**ï¼šæ¨¡å—åŒ–è®¾è®¡ã€å¼‚æ­¥æ‰§è¡Œã€å®¹å™¨åŒ–æ”¯æŒ
3. **å®ç”¨æ€§**ï¼šä¸°å¯Œçš„å·¥å…·ç”Ÿæ€ã€çµæ´»çš„é…ç½®ç³»ç»Ÿ
4. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰å·¥å…·å’Œæä¾›å•†æ‰©å±•

### 10.2 Node.js å®ç°ä¼˜åŠ¿

1. **æ€§èƒ½æå‡**ï¼šV8 å¼•æ“ä¼˜åŒ–ï¼Œå¼‚æ­¥ I/O æ€§èƒ½ä¼˜å¼‚
2. **ç±»å‹å®‰å…¨**ï¼šTypeScript æä¾›ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
3. **ç”Ÿæ€ä¸°å¯Œ**ï¼šNPM ç”Ÿæ€ç³»ç»Ÿæ”¯æŒ
4. **éƒ¨ç½²ä¾¿æ·**ï¼šå•ä¸€å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå®¹å™¨åŒ–å‹å¥½

é€šè¿‡æœ¬æŒ‡å—ï¼Œå¼€å‘è€…å¯ä»¥æ·±å…¥ç†è§£ Trae Agent çš„æŠ€æœ¯ç²¾é«“ï¼Œå¹¶æˆåŠŸå°†å…¶ç§»æ¤åˆ° Node.js å¹³å°ï¼Œä¸º JavaScript/TypeScript ç”Ÿæ€ç³»ç»Ÿå¸¦æ¥å¼ºå¤§çš„ AI ä»£ç†èƒ½åŠ›ã€‚
